{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of relevant responses and assigns to each region.\n",
    "def compute_simple_wha_aggregate(region_name, col_name, data):\n",
    "    admin_data = pd.DataFrame(columns=[region_name, 'Zero_Care_Row_Count', 'Zero_Care_SW_Count',\n",
    "                                       'Care_SW_Count', 'Care_Row_Count'])\n",
    "    \n",
    "    admin_data[region_name] = list(data[col_name].unique())\n",
    "    \n",
    "    # Set index to region numbers.   \n",
    "    wha_data = data.copy()\n",
    "    wha_data.set_index(col_name, inplace=True)\n",
    "    wha_data['M14$1'] = pd.to_numeric(wha_data['M14$1'], errors='coerce').fillna(-1).astype(int)\n",
    "    \n",
    "    # Set index to region numbers and sort index.\n",
    "    admin_data.set_index(region_name, inplace=True)\n",
    "    admin_data.fillna(0, inplace=True)\n",
    "    admin_data.sort_index(ascending=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    for index, row in wha_data.iterrows():\n",
    "        result = row['M14$1']\n",
    "        sample_weight = row['V005'] / 1000000\n",
    "        \n",
    "       #  Use the sample weights to summate the total number of antenatal care visits. Ignore missing values i.e. -1\n",
    "        if(result > 0 and result < 98):\n",
    "            prev = admin_data.loc[index, 'Care_SW_Count'] \n",
    "            admin_data.loc[index, 'Care_SW_Count'] = prev + (sample_weight * result)\n",
    "            \n",
    "            prev = admin_data.loc[index, 'Care_Row_Count']\n",
    "            admin_data.loc[index, 'Care_Row_Count'] = prev + result\n",
    "            \n",
    "        #  Increment the number of participants who have not had any antenatal care.\n",
    "        if(result == 0):\n",
    "            prev = admin_data.loc[index, 'Zero_Care_Row_Count']\n",
    "            admin_data.loc[index, 'Zero_Care_Row_Count'] = prev + 1\n",
    "            \n",
    "            prev = admin_data.loc[index, 'Zero_Care_SW_Count']\n",
    "            admin_data.loc[index, 'Zero_Care_SW_Count'] = prev + sample_weight\n",
    "         \n",
    "    return admin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns each region total number of positive responses which have been normalised by the population of the clusters.\n",
    "def wha_voronoi_aggregate(region_name, col_name, wha_recode, voronoi_data, population):\n",
    "    pop_data = population.copy()\n",
    "    vor_data = voronoi_data.copy() \n",
    "    wha_data = wha_recode.copy()\n",
    "\n",
    "    # Replace cells of no responses with -1.     \n",
    "    wha_data.set_index('V001', inplace=True)\n",
    "    wha_data['M14$1'] = pd.to_numeric(wha_data['M14$1'], errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "    # Contains aggregate for each cluster. \n",
    "    cluster_data = pd.DataFrame(columns=['V001','No_Care_Count_Aggregate','No_Care_SW_Aggregate',\n",
    "                                        'Care_Count_Aggregate', 'Care_SW_Aggregate'])\n",
    "    \n",
    "    cluster_data['V001'] = list(wha_recode['V001'].unique())\n",
    "\n",
    "    cluster_data.set_index('V001', inplace=True)\n",
    "    cluster_data.fillna(0, inplace=True)\n",
    "    cluster_data.sort_index(ascending=True, inplace=True)\n",
    "    \n",
    "    # Contains Admin Region and the women's access to health aggregates.\n",
    "    admin_region_data = pd.DataFrame(columns=[region_name, 'Care_Row_Count', 'Care_SW_Count', 'Care_Pop_Count',\n",
    "                                             'Zero_Row_Count', 'Zero_SW_Count','Zero_Pop_Count', 'Population'])\n",
    "    admin_region_data[region_name] = list(voronoi_data[col_name].unique())\n",
    "    admin_region_data.set_index(region_name, inplace=True)\n",
    "    admin_region_data.fillna(0, inplace=True)\n",
    "    admin_region_data.sort_index(ascending=True, inplace=True)\n",
    "    \n",
    "    for index, row in wha_data.iterrows():\n",
    "        value = row['M14$1']\n",
    "        sample_weight = row['V005'] / 1000000\n",
    "        \n",
    "        if(value > 0 and value < 98):\n",
    "            prev = cluster_data.loc[index, 'Care_SW_Aggregate']\n",
    "            cluster_data.loc[index, 'Care_SW_Aggregate'] = prev + (sample_weight * value)\n",
    "            \n",
    "            prev = cluster_data.loc[index, 'Care_Count_Aggregate']\n",
    "            cluster_data.loc[index, 'Care_Count_Aggregate'] = prev + value\n",
    "        \n",
    "        if(value == 0):\n",
    "            prev = cluster_data.loc[index, 'No_Care_Count_Aggregate']\n",
    "            cluster_data.loc[index,'No_Care_Count_Aggregate' ] = prev + 1\n",
    "            \n",
    "            prev = cluster_data.loc[index,'No_Care_SW_Aggregate']\n",
    "            cluster_data.loc[index, 'No_Care_SW_Aggregate'] = prev + sample_weight\n",
    "            \n",
    "            \n",
    "    cluster_data.reset_index(inplace=True)\n",
    "    \n",
    "    # Result of vor data is (cluster, region, proportion, SW_aggregate, Row_Aggregate, ..., ...).\n",
    "    vor_data = pd.merge(voronoi_data, cluster_data, how='inner', on='V001')\n",
    "    vor_data = pd.merge(vor_data, pop_data, how='inner', on='V001')\n",
    "    vor_data.reset_index(inplace=True)\n",
    "    vor_data.set_index(col_name, inplace=True)\n",
    "    vor_data.sort_index(ascending=True, inplace=True)\n",
    "    \n",
    "    # Assigns responses to regions based on voronoi overlap with region. Proportions cluster population in same way. \n",
    "    for ind, row in vor_data.iterrows():\n",
    "        care_count_prop = row['Proportion'] * row['Care_Count_Aggregate']\n",
    "        prev = admin_region_data.loc[ind,'Care_Row_Count']\n",
    "        admin_region_data.loc[ind, 'Care_Row_Count'] = prev + care_count_prop\n",
    "        \n",
    "        care_sw_prop = row['Proportion'] * row['Care_SW_Aggregate']\n",
    "        prev = admin_region_data.loc[ind, 'Care_SW_Count']\n",
    "        admin_region_data.loc[ind, 'Care_SW_Count'] = prev + care_sw_prop\n",
    "        \n",
    "        zero_count_prop = row['Proportion'] * row['No_Care_Count_Aggregate']\n",
    "        prev = admin_region_data.loc[ind,'Zero_Row_Count']\n",
    "        admin_region_data.loc[ind, 'Zero_Row_Count'] = prev + zero_count_prop\n",
    "        \n",
    "        zero_sw_prop = row['Proportion'] * row['No_Care_SW_Aggregate']\n",
    "        prev = admin_region_data.loc[ind, 'Zero_SW_Count']\n",
    "        admin_region_data.loc[ind, 'Zero_SW_Count'] = prev + zero_sw_prop \n",
    "        \n",
    "        prop = row['Proportion'] * row['Population']\n",
    "        prev = admin_region_data.loc[ind, 'Population']\n",
    "        admin_region_data.loc[ind, 'Population'] = prev + prop \n",
    "    \n",
    "    #  Normalises total responses in region using population of all clusters in the region.\n",
    "    for ind, row in admin_region_data.iterrows():\n",
    "        value = row['Zero_Row_Count'] /  row['Population']\n",
    "        admin_region_data.loc[ind, 'Zero_Pop_Count'] = value * (10**6)\n",
    "        \n",
    "        value = row['Care_Row_Count'] /  row['Population']\n",
    "        admin_region_data.loc[ind, 'Care_Pop_Count'] = value * (10**6)\n",
    "    return admin_region_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------HOW TO AGGREGATE-----------------\n",
      "-----NUMBER OF ANTENATAL VISITS-------\n",
      "Admin 1 - (Visits row count VOR, Visits count PIP): 0.9064327485380117\n",
      "Admin 2 - (Visits row count VOR, Visits count PIP): 0.80147085571754906\n",
      "\n",
      "----NUMBER OF ZER0 ANTENATAL VISITS-----\n",
      "Admin 1 - (Zero Care row count VOR, Zero Care count PIP): 0.92354539019687687\n",
      "Admin 2 - (Zero Care row count VOR, Zero Care count PIP): 0.86106102394459927\n",
      "\n",
      "------------------HOW TO COUNT-------------------\n",
      "------NUMBER OF ANTENATAL VISITS-------\n",
      "Admin 1 - (Visits row count VOR, Visits Sample Weight VOR): 0.48538011695906425\n",
      "Admin 1 - (Visits row count VOR, Visits pop VOR): 0.391812865497076\n",
      "\n",
      "Admin 2 - (Visits row count VOR, Visits Sample Weight VOR): 0.48244897959183675\n",
      "Admin 2 - (Visits row count VOR, Visits pop VOR): 0.29959183673469386\n",
      "\n",
      "Admin 3 - (Visits row count VOR, Visits Sample Weight VOR): 0.59964747356051706\n",
      "Admin 3 - (Visits row count VOR, Visits pop VOR): 0.45757931844888367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------COTE D'IVOIRE COMPUTATIONS--------------------------\n",
    "recode_path = \"Cote D'Ivoire Data SPSS/ciir62sv/\"\n",
    "path_to_voronoi = \"Cote D'Ivoire Data SPSS/Voronoi clusters/Proportions/\"\n",
    "path_to_clusters = \"Cote D'Ivoire Data SPSS/CLUSTER_TO_REGION/\"\n",
    "\n",
    "# Population of all DHS clusters.\n",
    "population_file = pd.read_csv(\"Cote D'Ivoire Data SPSS/Region_Population/voronoi.csv\")\n",
    "population_file = population_file.rename(columns = {'DHSCLUST':'V001'})\n",
    "population_file = population_file.rename(columns = {'_count':'Population'})\n",
    "\n",
    "file = pd.read_csv(recode_path + \"individual_Recode.csv\", usecols=['V001','V002','V005','M14$1'])\n",
    "\n",
    "clust_admin = pd.read_csv(path_to_clusters + \"admin_3.csv\", usecols=['DHSCLUST', 'ID_1', 'ID_2', 'ID_3'])\n",
    "clust_admin = clust_admin.rename(columns = {'DHSCLUST':'V001'})\n",
    "\n",
    "# Voronoi Polygon & Admin Region Mapping.\n",
    "voronoi_admin_1 = pd.read_csv(path_to_voronoi + \"voronoi_admin_1.csv\")\n",
    "voronoi_admin_2 = pd.read_csv(path_to_voronoi + \"voronoi_admin_2.csv\")\n",
    "voronoi_admin_3 = pd.read_csv(path_to_voronoi + \"voronoi_admin_3.csv\")\n",
    "\n",
    "voronoi_admin_1 = voronoi_admin_1.rename(columns = {'DHSCLUST':'V001'})\n",
    "voronoi_admin_2 = voronoi_admin_2.rename(columns = {'DHSCLUST':'V001'})\n",
    "voronoi_admin_3 = voronoi_admin_3.rename(columns = {'DHSCLUST':'V001'})\n",
    "\n",
    "# Join the dataframes. \n",
    "clust_file = pd.merge(file, clust_admin, how='inner', on='V001')\n",
    "\n",
    "# ---------------------------SIMPLE AGGREGATES USING POINT IN POLYGON-------------------------------------\n",
    "admin_1 = compute_simple_wha_aggregate(\"Admin_1_Region\", \"ID_1\", clust_file)\n",
    "admin_2 = compute_simple_wha_aggregate(\"Admin_2_Region\", \"ID_2\", clust_file)\n",
    "admin_3 = compute_simple_wha_aggregate(\"Admin_3_Region\", \"ID_3\", clust_file)\n",
    "admin_1.to_csv(recode_path + \"wha_aggregate_1.csv\", index=True)\n",
    "admin_2.to_csv(recode_path + \"wha_aggregate_2.csv\", index=True)\n",
    "admin_3.to_csv(recode_path + \"wha_aggregate_3.csv\", index=True)\n",
    "\n",
    "# ---------------------------AGGREGATES USING VORONOI CALCULATIONS-------------------------------------\n",
    "admin_vor_1 = wha_voronoi_aggregate(\"Admin_Region_1\", \"ID_1\", clust_file, voronoi_admin_1, population_file)\n",
    "admin_vor_2 = wha_voronoi_aggregate(\"Admin_Region_2\", \"ID_2\", clust_file, voronoi_admin_2, population_file)\n",
    "admin_vor_3 = wha_voronoi_aggregate(\"Admin_Region_3\", \"ID_3\", clust_file, voronoi_admin_3, population_file)\n",
    "\n",
    "print(\"---------------------HOW TO AGGREGATE-----------------\")\n",
    "print(\"-----NUMBER OF ANTENATAL VISITS-------\")\n",
    "x, y = stats.kendalltau(admin_vor_1['Care_Row_Count'], admin_1['Care_Row_Count'])\n",
    "print(\"Admin 1 - (Visits row count VOR, Visits count PIP): \" +  repr(x))\n",
    "\n",
    "x, y = stats.kendalltau(admin_vor_2['Care_Row_Count'], admin_2['Care_Row_Count'])\n",
    "print(\"Admin 2 - (Visits row count VOR, Visits count PIP): \" +  repr(x) + \"\\n\")\n",
    "\n",
    "print(\"----NUMBER OF ZER0 ANTENATAL VISITS-----\")\n",
    "x, y = stats.kendalltau(admin_vor_1['Zero_Row_Count'], admin_1['Zero_Care_Row_Count'])\n",
    "print(\"Admin 1 - (Zero Care row count VOR, Zero Care count PIP): \" +  repr(x))\n",
    "\n",
    "x, y = stats.kendalltau(admin_vor_2['Zero_Row_Count'], admin_2['Zero_Care_Row_Count'])\n",
    "print(\"Admin 2 - (Zero Care row count VOR, Zero Care count PIP): \" +  repr(x) + \"\\n\")\n",
    "\n",
    "print(\"------------------HOW TO COUNT-------------------\")\n",
    "print(\"------NUMBER OF ANTENATAL VISITS-------\")\n",
    "x, y = stats.kendalltau(admin_vor_1['Care_Row_Count'], admin_vor_1['Care_SW_Count'])\n",
    "print(\"Admin 1 - (Visits row count VOR, Visits Sample Weight VOR): \" + repr(x))\n",
    "\n",
    "x, y = stats.kendalltau(admin_vor_1['Care_Row_Count'], admin_vor_1['Care_Pop_Count'])\n",
    "print(\"Admin 1 - (Visits row count VOR, Visits pop VOR): \" + repr(x) + \"\\n\")\n",
    "\n",
    "x, y = stats.kendalltau(admin_vor_2['Care_Row_Count'], admin_vor_2['Care_SW_Count'])\n",
    "print(\"Admin 2 - (Visits row count VOR, Visits Sample Weight VOR): \" + repr(x))\n",
    "\n",
    "x, y = stats.kendalltau(admin_vor_2['Care_Row_Count'], admin_vor_2['Care_Pop_Count'])\n",
    "print(\"Admin 2 - (Visits row count VOR, Visits pop VOR): \" + repr(x) + \"\\n\")\n",
    "\n",
    "x, y = stats.kendalltau(admin_vor_3['Care_Row_Count'], admin_vor_3['Care_SW_Count'])\n",
    "print(\"Admin 3 - (Visits row count VOR, Visits Sample Weight VOR): \" + repr(x))\n",
    "\n",
    "x, y = stats.kendalltau(admin_vor_3['Care_Row_Count'], admin_vor_3['Care_Pop_Count'])\n",
    "print(\"Admin 3 - (Visits row count VOR, Visits pop VOR): \" + repr(x) + \"\\n\")\n",
    "\n",
    "admin_vor_1.to_csv(recode_path + \"wha_vor_aggregate_1.csv\", index=True)\n",
    "admin_vor_2.to_csv(recode_path + \"wha_vor_aggregate_2.csv\", index=True)\n",
    "admin_vor_3.to_csv(recode_path + \"wha_vor_aggregate_3.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'SN_2012-13/Region_Populations/voronoi_pop.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-22f2cf39bd44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Population of all DHS clusters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpopulation_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SN_2012-13/Region_Populations/voronoi_pop.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpopulation_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'DHSCLUST'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'V001'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpopulation_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'_count'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Population'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'SN_2012-13/Region_Populations/voronoi_pop.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#  ------------------------------------SENEGAL COMPUTATIONS--------------------------------------------------\n",
    "recode_path = \"SN_2012-13/SNIR6DSV/\"\n",
    "path_to_voronoi = \"SN_2012-13/Voronoi/Proportions/\"\n",
    "path_to_clusters = \"SN_2012-13/Cluster_To_Region/\"\n",
    "\n",
    "# Population of all DHS clusters.\n",
    "population_file = pd.read_csv(\"SN_2012-13/Region_Populations/voronoi_pop.csv\")\n",
    "population_file = population_file.rename(columns = {'DHSCLUST':'V001'})\n",
    "population_file = population_file.rename(columns = {'_count':'Population'})\n",
    "\n",
    "file = pd.read_csv(recode_path + \"individual_recode.csv\", usecols=['V001','V002','V005','M14$1'])\n",
    "\n",
    "clust_admin = pd.read_csv(path_to_clusters + \"admin_3.csv\", usecols=['DHSCLUST', 'ID_1', 'ID_2', 'ID_3'])\n",
    "clust_admin = clust_admin.rename(columns = {'DHSCLUST':'V001'})\n",
    "\n",
    "# Voronoi Polygon & Admin Region Mapping.\n",
    "voronoi_admin_1 = pd.read_csv(path_to_voronoi + \"voronoi_admin_1.csv\")\n",
    "voronoi_admin_2 = pd.read_csv(path_to_voronoi + \"voronoi_admin_2.csv\")\n",
    "voronoi_admin_3 = pd.read_csv(path_to_voronoi + \"voronoi_admin_3.csv\")\n",
    "\n",
    "voronoi_admin_1 = voronoi_admin_1.rename(columns = {'DHSCLUST':'V001'})\n",
    "voronoi_admin_2 = voronoi_admin_2.rename(columns = {'DHSCLUST':'V001'})\n",
    "voronoi_admin_3 = voronoi_admin_3.rename(columns = {'DHSCLUST':'V001'})\n",
    "\n",
    "clust_file = pd.merge(file, clust_admin, how='inner', on='V001')\n",
    "\n",
    "#  Set empty cells to -1.   \n",
    "clust_file['M14$1'] = pd.to_numeric(clust_file['M14$1'], errors='coerce').fillna(-1).astype(int)\n",
    "clust_file.to_csv(recode_path + \"complete_health_access_data.csv\", index=False)\n",
    "\n",
    "admin_vor_1 = wha_voronoi_aggregate(\"Admin_Region_1\", \"ID_1\", clust_file, voronoi_admin_1, pop)\n",
    "admin_vor_2 = wha_voronoi_aggregate(\"Admin_Region_2\", \"ID_2\", clust_file, voronoi_admin_2, pop)\n",
    "admin_vor_3 = wha_voronoi_aggregate(\"Admin_Region_3\", \"ID_3\", clust_file, voronoi_admin_3, pop)\n",
    "\n",
    "admin_vor_1.to_csv(recode_path + \"wha_vor_aggregate_1.csv\", index=True)\n",
    "admin_vor_2.to_csv(recode_path + \"wha_vor_aggregate_2.csv\", index=True)\n",
    "admin_vor_3.to_csv(recode_path + \"wha_vor_aggregate_3.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
